{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Training and testing PumpkinLLM Pre-Alpha notebook"
      ],
      "metadata": {
        "id": "MYrqQ8HVurg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Предисловие\n",
        "\n",
        "В этом ноутбуке я обучу и протестирую свою собственную GPT модель реализованную на [PyTorch](\"https://pytorch.org/\"). В качестве датасета я буду использовать [набор данных](\"https://www.kaggle.com/datasets/thedevastator/dailydialog-unlock-the-conversation-potential-in\") которые я нашел на просторах [Kaggle](\"https://www.kaggle.com/\"). Целью данного ноутбука была проверка работоспобности GPT, которую она успешно подтвердила. Именно поэтому я взял супер маленький размер словаря, а так же всего 3 эпохи обучения GPT."
      ],
      "metadata": {
        "id": "cVik20c-uyNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Работа с данными"
      ],
      "metadata": {
        "id": "IUTncY8DwEPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем все библиотеки которые понадобятся для обучения и тестирования модели."
      ],
      "metadata": {
        "id": "241XNC0mwPtk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plZkEWZJSobW",
        "outputId": "06cbd952-ae78-4195-b19f-546b337e8c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.39 s, sys: 430 ms, total: 3.82 s\n",
            "Wall time: 6.58 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np, pandas as pd, torch, kagglehub, os, sys, json\n",
        "from tqdm import tqdm\n",
        "from getpass import getpass\n",
        "from torch.utils.data import Dataset\n",
        "from collections import Counter, defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем данные а так же смотрим их содержимое."
      ],
      "metadata": {
        "id": "ey_ODTQtxXql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"thedevastator/dailydialog-unlock-the-conversation-potential-in\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zp5WqAYSw5q",
        "outputId": "6b744b3e-cea7-4f64-8d21-689a032508d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'dailydialog-unlock-the-conversation-potential-in' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "data_vak = pd.read_csv(os.path.join(path, 'validation.csv'))\n",
        "data_test = pd.read_csv(os.path.join(path, 'test.csv'))"
      ],
      "metadata": {
        "id": "zLVwWXsxTf8U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "pD5eABxWUIdh",
        "outputId": "80fcdb19-9117-41d2-937b-c80f4d2d750b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              dialog                    act  \\\n",
              "0  ['Say , Jim , how about going for a few beers ...  [3 4 2 2 2 3 4 1 3 4]   \n",
              "1  ['Can you do push-ups ? '\\n \" Of course I can ...          [2 1 2 2 1 1]   \n",
              "\n",
              "                 emotion  \n",
              "0  [0 0 0 0 0 0 4 4 4 4]  \n",
              "1          [0 0 6 0 0 0]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e50ea07-9eb6-45af-8186-e45cfc22bf51\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dialog</th>\n",
              "      <th>act</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>['Say , Jim , how about going for a few beers ...</td>\n",
              "      <td>[3 4 2 2 2 3 4 1 3 4]</td>\n",
              "      <td>[0 0 0 0 0 0 4 4 4 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['Can you do push-ups ? '\\n \" Of course I can ...</td>\n",
              "      <td>[2 1 2 2 1 1]</td>\n",
              "      <td>[0 0 6 0 0 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e50ea07-9eb6-45af-8186-e45cfc22bf51')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8e50ea07-9eb6-45af-8186-e45cfc22bf51 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8e50ea07-9eb6-45af-8186-e45cfc22bf51');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5b742ea9-64da-4b45-acfa-f3ea1a2748a9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b742ea9-64da-4b45-acfa-f3ea1a2748a9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5b742ea9-64da-4b45-acfa-f3ea1a2748a9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_train",
              "summary": "{\n  \"name\": \"data_train\",\n  \"rows\": 11118,\n  \"fields\": [\n    {\n      \"column\": \"dialog\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10549,\n        \"samples\": [\n          \"[\\\"How's your brother doing ? \\\"\\n \\\" As a matter of fact , he hasn't been feeling too well . \\\"\\n \\\" I'm sorry to hear that . What's the matter ? \\\"\\n \\\" Tell him I hope he's better soon . \\\"\\n \\\" I'll tell him . Thanks for asking about him . \\\"]\",\n          \"[\\\"just don't understand why we have to take the subway . Look at this place . It's modern enough . But it's far from lively . \\\"\\n \\\" It's convenient . People in network go everywhere by subway if they don't drive . \\\"\\n \\\" This isn't network , Frida . We are in San Francisco . \\\"\\n \\\" What's the difference ? \\\"\\n \\\" We haven't come here in business but for pleasure . We should go out exciting . What can you see in the subway ? Nothing ! \\\"\\n ' What can you see from a tram or a double-decker ? '\\n ' I can see people who are walking on the sidewalk . I can see cars driving on the streets . I can see the sun shining and beautiful flowers dancing in the wind . I can even smell the scent of flowers and the freshness of the air . '\\n ' But they are not as convenient as the subway . '\\n ' You miss the point ! We are not in a hurry to go anywhere . '\\n \\\" Well , maybe you are right . We'll be more amused in a tram or a double-decker than on the subway . \\\"\\n \\\" I'm so glad you can finally see things in my way . Can we leave this dull place now ? \\\"\\n \\\" What about the tickets ? I've already bought them . \\\"\\n ' Just throw them away ! ']\",\n          \"['Good evening , sir . What can I do for you ? '\\n ' We are going to have a party tonight , would you please look after our baby tonight ? '\\n ' Well , we have baby-sitting service . We will take good care of your baby . '\\n \\\" That's good . \\\" ' Please fill out the table . ']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"act\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5005,\n        \"samples\": [\n          \"[3 4 2 3 2 2 1 2 1 2 1]\",\n          \"[3 3 2 1 4 2 1 2 1 2 1 2 1 2 1 2 1 3]\",\n          \"[3 2 3 2 1 3 3 4]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"emotion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2453,\n        \"samples\": [\n          \"[0 0 0 0 0 5 0 0 0]\",\n          \"[1 0 0]\",\n          \"[4 4 4 4 4 4 4 0 0 0 0 0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим как выглядит пример диалога."
      ],
      "metadata": {
        "id": "_Lo8KK2xxrzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train['dialog'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "vUK4_k9cWpxd",
        "outputId": "476dbaad-a172-46ec-d717-cfdfc23f66e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[\\'Say , Jim , how about going for a few beers after dinner ? \\'\\n \\' You know that is tempting but is really not good for our fitness . \\'\\n \\' What do you mean ? It will help us to relax . \\'\\n \" Do you really think so ? I don\\'t . It will just make us fat and act silly . Remember last time ? \"\\n \" I guess you are right.But what shall we do ? I don\\'t feel like sitting at home . \"\\n \\' I suggest a walk over to the gym where we can play singsong and meet some of our friends . \\'\\n \" That\\'s a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . \"\\n \\' Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . \\'\\n \" Good.Let \\' s go now . \" \\' All right . \\']'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь почистим диалог, добавим все это в массив корпуса и сохрнаним корпус как текстовый файл."
      ],
      "metadata": {
        "id": "ETR0C_NdxwJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = list()\n",
        "for dialog in data_train['dialog']:\n",
        "  dialog = eval(dialog)\n",
        "  text = \"<EOS>\".join(dialog)\n",
        "  corpus.append(text)\n",
        "print(corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwE8CBvnjQeo",
        "outputId": "9d5f18ec-f26e-42dd-f360-5418467af786"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Say , Jim , how about going for a few beers after dinner ?  You know that is tempting but is really not good for our fitness .  What do you mean ? It will help us to relax .  Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ?  I guess you are right.But what shall we do ? I don't feel like sitting at home .  I suggest a walk over to the gym where we can play singsong and meet some of our friends .  That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them .  Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too .  Good.Let ' s go now .  All right . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('dialog.txt', 'w', encoding='utf-8') as f:\n",
        "  f.write('\\n\\n'.join(corpus))"
      ],
      "metadata": {
        "id": "MszeSOHZjrVt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Обучение токенизатора"
      ],
      "metadata": {
        "id": "4X4Sf8EeyLo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, vocab_size, end_flag=True):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.end_flag = end_flag\n",
        "\n",
        "        self.vocab = set()\n",
        "        self.token2id = dict()\n",
        "        self.id2token = dict()\n",
        "        self.merges = list()\n",
        "\n",
        "        self.pad_token = '[PAD]'\n",
        "        self.unk_token = '[UNK]'\n",
        "        self.bos_token = '[BOS]'\n",
        "        self.eos_token = '[EOS]'\n",
        "\n",
        "        self.special_tokens = [\n",
        "            self.pad_token,\n",
        "            self.unk_token,\n",
        "            self.bos_token,\n",
        "            self.eos_token\n",
        "        ]\n",
        "\n",
        "        self.end_marker = '</w>' if self.end_flag else None\n",
        "        self._trained = False\n",
        "\n",
        "    def _prepare_corpus(self, corpus):\n",
        "        prepared_corpus = list()\n",
        "        for w in corpus:\n",
        "            if self.end_flag: prepared_corpus.append(list(w) + [self.end_marker])\n",
        "            else: prepared_corpus.append(list(w))\n",
        "        return prepared_corpus\n",
        "\n",
        "    def _get_pair_stats(self, corpus):\n",
        "        pairs = Counter()\n",
        "        for word in corpus:\n",
        "            for i in range(len(word) - 1):\n",
        "                pairs[(word[i], word[i + 1])] += 1\n",
        "        return pairs\n",
        "\n",
        "    def _merge_corpus(self, corpus, pair):\n",
        "        merged_token = pair[0] + pair[1]\n",
        "        new_corpus = list()\n",
        "\n",
        "        for word in corpus:\n",
        "            new_word = list()\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                if i < len(word) - 1 and word[i] == pair[0] and word[i + 1] == pair[1]:\n",
        "                    new_word.append(merged_token)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_corpus.append(new_word)\n",
        "        return new_corpus\n",
        "\n",
        "    def train(self, corpus):\n",
        "        if len(corpus) == 0: raise ValueError(\"Corpus us empty\")\n",
        "\n",
        "        corpus_sample = corpus[:20000]\n",
        "\n",
        "        for word in corpus_sample:\n",
        "            for ch in word:\n",
        "                self.vocab.add(ch)\n",
        "\n",
        "        for st in self.special_tokens: self.vocab.add(st)\n",
        "\n",
        "        if self.end_flag: self.vocab.add(self.end_marker)\n",
        "\n",
        "        corpus = self._prepare_corpus(corpus_sample)\n",
        "        initial_size = len(self.vocab)\n",
        "        remaining = self.vocab_size - initial_size\n",
        "\n",
        "        if remaining <= 0:\n",
        "            self._finalize_vocab()\n",
        "            self._trained = True\n",
        "            return corpus_sample, self.vocab\n",
        "\n",
        "        bar = tqdm(total=remaining, desc='Training Tokenizer')\n",
        "\n",
        "        while len(self.vocab) < self.vocab_size:\n",
        "            pair_stats = self._get_pair_stats(corpus)\n",
        "            if not pair_stats: break\n",
        "\n",
        "            best_pair = pair_stats.most_common(1)[0][0]\n",
        "            new_token = ''.join(best_pair)\n",
        "\n",
        "            if new_token in self.vocab:\n",
        "                del pair_stats[best_pair]\n",
        "                if not pair_stats:\n",
        "                    break\n",
        "                best_pair = pair_stats.most_common(1)[0][0]\n",
        "                new_token = ''.join(best_pair)\n",
        "                if new_token in self.vocab: break\n",
        "\n",
        "            corpus = self._merge_corpus(corpus, best_pair)\n",
        "            self.vocab.add(new_token)\n",
        "            self.merges.append(best_pair)\n",
        "\n",
        "            bar.update(1)\n",
        "            if bar.n >= bar.total: break\n",
        "\n",
        "        bar.close()\n",
        "        self._finalize_vocab()\n",
        "        self._trained = True\n",
        "\n",
        "        return corpus_sample, self.vocab\n",
        "\n",
        "    def _finalize_vocab(self):\n",
        "        sorted_vocab = sorted(self.vocab)\n",
        "        self.token2id = {tok: i for i, tok in enumerate(sorted_vocab)}\n",
        "        self.id2token = {i: tok for tok, i in self.token2id.items()}\n",
        "\n",
        "        if self.unk_token not in self.token2id:\n",
        "            unk_id = len(self.token2id)\n",
        "            self.token2id[self.unk_token] = unk_id\n",
        "            self.id2token[unk_id] = self.unk_token\n",
        "\n",
        "    def encode(self, text, max_len=None, add_bos=True, add_eos=True):\n",
        "        if not self._trained: raise RuntimeError(\"Tokenizer is not trained\")\n",
        "\n",
        "        words = text.split()\n",
        "        tokens = list()\n",
        "\n",
        "        for w in words:\n",
        "            chars = list(w)\n",
        "            if self.end_flag: chars.append(self.end_marker)\n",
        "\n",
        "            changed_flag = True\n",
        "            while changed_flag:\n",
        "                changed_flag = False\n",
        "                i = 0\n",
        "                while i < len(chars) - 1:\n",
        "                    pair = (chars[i], chars[i + 1])\n",
        "                    if pair in self.merges:\n",
        "                        merged = ''.join(pair)\n",
        "                        chars[i:i + 2] = [merged]\n",
        "                        changed_flag = True\n",
        "                    else: i += 1\n",
        "\n",
        "            tokens.extend(chars)\n",
        "\n",
        "        if add_bos: tokens = [self.bos_token] + tokens\n",
        "        if add_eos: tokens += [self.eos_token]\n",
        "\n",
        "        ids = [self.token2id.get(t, self.token2id[self.unk_token]) for t in tokens]\n",
        "\n",
        "        if max_len is not None:\n",
        "            if len(ids) > max_len: ids = ids[:max_len]\n",
        "            else:\n",
        "                pad_id = self.token2id[self.pad_token]\n",
        "                ids += [pad_id] * (max_len - len(ids))\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, token_ids, skip_special=True):\n",
        "        if not self.id2token: raise RuntimeError(\"Vocab is empty\")\n",
        "\n",
        "        tokens = [self.id2token.get(i, self.unk_token) for i in token_ids]\n",
        "        out_tokens = list()\n",
        "\n",
        "        for t in tokens:\n",
        "            if skip_special and (t in self.special_tokens): continue\n",
        "            if self.end_flag and t == self.end_marker: continue\n",
        "            if self.end_flag and self.end_marker in t: t = t.replace(self.end_marker, ' ')\n",
        "            out_tokens.append(t)\n",
        "\n",
        "        text = ' '.join(out_tokens)\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def batch_encode(self, texts, max_len=None, add_bos=True, add_eos=True):\n",
        "        batch = [\n",
        "            self.encode(t, max_len=max_len, add_bos=add_bos, add_eos=add_eos)\n",
        "            for t in texts\n",
        "        ]\n",
        "        return torch.tensor(batch, dtype=torch.long)\n",
        "\n",
        "    def save(self, path):\n",
        "        data = {\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'end_flag': self.end_flag,\n",
        "            'token2id': self.token2id,\n",
        "            'id2token': self.id2token,\n",
        "            'merges': self.merges,\n",
        "            'special_tokens': {\n",
        "                'pad': self.pad_token,\n",
        "                'unk': self.unk_token,\n",
        "                'bos': self.bos_token,\n",
        "                'eos': self.eos_token,\n",
        "                'end_marker': self.end_marker\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, ensure_ascii=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        with open(path, 'r', encoding='utf-8') as f: data = json.load(f)\n",
        "\n",
        "        tokenizer = cls(\n",
        "            vocab_size=data['vocab_size'],\n",
        "            end_flag=data['end_flag']\n",
        "        )\n",
        "\n",
        "        tokenizer.token2id = data['token2id']\n",
        "        tokenizer.id2token = {int(k): v for k, v in data['id2token'].items()}\n",
        "        tokenizer.merges = [tuple(x) for x in data['merges']]\n",
        "\n",
        "        tokenizer.pad_token = data['special_tokens']['pad']\n",
        "        tokenizer.unk_token = data['special_tokens']['unk']\n",
        "        tokenizer.bos_token = data['special_tokens']['bos']\n",
        "        tokenizer.eos_token = data['special_tokens']['eos']\n",
        "        tokenizer.end_marker = data['special_tokens']['end_marker']\n",
        "\n",
        "        tokenizer.vocab = set(tokenizer.token2id.keys())\n",
        "        tokenizer._trained = True\n",
        "\n",
        "        return tokenizer"
      ],
      "metadata": {
        "id": "s9slYvWnBf1w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция обучения токенизатора и обучение"
      ],
      "metadata": {
        "id": "RpHuKxACyY26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(input_path, save_dir, vocab_size, seq_len):\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read().splitlines()\n",
        "\n",
        "    tokenizer = Tokenizer(vocab_size=vocab_size)\n",
        "    tokenizer.train(text)\n",
        "    tokenizer.save(os.path.join(save_dir, 'tokenizer.json'))\n",
        "\n",
        "    data_ids = [tokenizer.encode(t, max_len=seq_len, add_bos=True, add_eos=True) for t in text]\n",
        "    torch.save(data_ids, os.path.join(save_dir, 'dataset.pt'))\n",
        "    print('ready!')"
      ],
      "metadata": {
        "id": "-1vJHFsgD1Qa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_data('/content/dialog.txt', '/content', 150, 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY6n6VxtEeWr",
        "outputId": "bbd30125-6ebd-45dd-eb66-48a6aa09746b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Tokenizer: 100%|██████████| 46/46 [01:47<00:00,  2.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка работы токенизатора"
      ],
      "metadata": {
        "id": "M7vsUIhn6OOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.load('/content/tokenizer.json')\n",
        "sample_text = \"Hello, how are you?\"\n",
        "encoded = tokenizer.encode(sample_text, max_len=20, add_bos=True, add_eos=True)\n",
        "decoded = tokenizer.decode(encoded, skip_special=True)\n",
        "\n",
        "print(\"Encoded:\", encoded)\n",
        "print(\"Decoded:\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL89WW9gFUCo",
        "outputId": "8c2e395d-4c5a-43f5-a3df-08ea0d379760"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded: [64, 44, 79, 101, 107, 12, 31, 90, 113, 31, 73, 79, 31, 133, 33, 31, 65, 66, 66, 66]\n",
            "Decoded: H e ll o , h ow ar e you ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение GPT"
      ],
      "metadata": {
        "id": "0Ur8LoEq6fZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mask(seq):\n",
        "    pad_mask = (seq == 0)\n",
        "    attention_mask = torch.triu(torch.ones((seq.size(1), seq.size(1)), device=seq.device), diagonal=1).bool()\n",
        "    return pad_mask, attention_mask\n",
        "\n",
        "class DecoderBlock(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, head_count, ffl_dim, dropout_rate):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.MultiHeadAttention = torch.nn.MultiheadAttention(embedding_dim,head_count, dropout_rate, batch_first=True)\n",
        "        self.ffl = torch.nn.Sequential(\n",
        "            torch.nn.Linear(embedding_dim, ffl_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(ffl_dim, embedding_dim),\n",
        "        )\n",
        "        self.normal_1 = torch.nn.LayerNorm(embedding_dim)\n",
        "        self.normal_2 = torch.nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.dropout_1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.dropout_2 = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, X, padding_mask, attention_mask):\n",
        "        mha, _ = self.MultiHeadAttention(X, X, X, attn_mask=attention_mask, key_padding_mask=padding_mask)\n",
        "        X = X + self.dropout_1(mha)\n",
        "        X = self.normal_1(X)\n",
        "\n",
        "        ffl = self.ffl(X)\n",
        "        X = X + self.dropout_2(ffl)\n",
        "        X = self.normal_2(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, max_len, embedding_dim):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.register_buffer(\"pos_encoding\", self.positional_encoding(max_len, embedding_dim))\n",
        "\n",
        "    def positional_encoding(self, max_len, embedding_dim):\n",
        "        positions = torch.arange(max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        index = torch.arange(embedding_dim, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        args = positions / torch.pow(10_000, (2 * torch.floor(index / 2)) / embedding_dim)\n",
        "\n",
        "        args[:, 0::2] = torch.sin(args[:, 0::2])\n",
        "        args[:, 1::2] = torch.cos(args[:, 1::2])\n",
        "\n",
        "        return args.unsqueeze(0)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X + self.pos_encoding[:, :X.size(1), :].to(X)\n",
        "\n",
        "class GPT(torch.nn.Module):\n",
        "    def __init__(self, layers_count, embedding_dim, head_count, ffl_dim, dropout_rate, vocab_size, max_len):\n",
        "        super(GPT, self).__init__()\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_encoding = PositionalEncoding(max_len, embedding_dim)\n",
        "        self.final = torch.nn.Linear(embedding_dim, vocab_size)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.decoder = torch.nn.ModuleList(\n",
        "            [DecoderBlock(embedding_dim, head_count, ffl_dim, dropout_rate) for _ in range(layers_count)]\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        padding_mask, attention_mask = build_mask(X)\n",
        "\n",
        "        X = self.embedding(X)\n",
        "        X = self.positional_encoding(X)\n",
        "        X = self.dropout(X)\n",
        "        for dec in self.decoder:\n",
        "            X = dec(X, padding_mask, attention_mask)\n",
        "\n",
        "        return self.final(X)"
      ],
      "metadata": {
        "id": "WyKvJuJdHNV9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание датасета для обучения GPT"
      ],
      "metadata": {
        "id": "cGJpID9W6Wg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, seq_len):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ids = torch.tensor(self.data[idx], dtype=torch.long)\n",
        "        return ids[:-1], ids[1:]"
      ],
      "metadata": {
        "id": "ewHaNrrByriO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Класс обучения GPT и само обучение"
      ],
      "metadata": {
        "id": "I8mZM2DA6ZvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(epochs, seq_len, batch_size, save_path):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    tokenizer = Tokenizer.load('/content/tokenizer.json')\n",
        "    data = torch.load('/content/dataset.pt')\n",
        "\n",
        "    dataset = TextDataset(data, seq_len=seq_len)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = GPT(\n",
        "        layers_count=2,\n",
        "        embedding_dim=128,\n",
        "        head_count=4,\n",
        "        ffl_dim=512,\n",
        "        dropout_rate=0.1,\n",
        "        vocab_size=len(tokenizer.vocab),\n",
        "        max_len=seq_len\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        loop = tqdm(loader, desc=f\"Epoch {epoch}\", leave=True)\n",
        "        for batch_idx, (x, y) in enumerate(loop):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        print(f\"Epoch {epoch} finished. Average loss: {avg_loss:.4f}\")\n",
        "\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        torch.save(model.state_dict(), os.path.join(save_path, f'gpt_epoch{epoch}.pt'))"
      ],
      "metadata": {
        "id": "t29qX39fHy_K"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(3, 64, 20, '/content')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPgS2P4NIpD0",
        "outputId": "4ff1bc58-227f-408c-922d-fb8246809a08"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 1112/1112 [02:25<00:00,  7.63it/s, loss=0.791]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 finished. Average loss: 1.3031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 1112/1112 [02:26<00:00,  7.57it/s, loss=0.872]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 finished. Average loss: 1.0585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 1112/1112 [02:27<00:00,  7.52it/s, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 finished. Average loss: 0.9726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Тестирование работы GPT"
      ],
      "metadata": {
        "id": "FSXD1QMX6tCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, model_path, tokenizer_path, max_len=64, temperature=1.0):\n",
        "    tokenizer = Tokenizer.load(tokenizer_path)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    model = GPT(\n",
        "        layers_count=2,\n",
        "        embedding_dim=128,\n",
        "        head_count=4,\n",
        "        ffl_dim=512,\n",
        "        dropout_rate=0.1,\n",
        "        vocab_size=len(tokenizer.vocab),\n",
        "        max_len=max_len\n",
        "    ).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, max_len=max_len, add_bos=True, add_eos=False)\n",
        "    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "\n",
        "            input_ids_truncated = input_ids[:, -max_len:]\n",
        "\n",
        "            logits = model(input_ids_truncated)\n",
        "            next_logits = logits[:, -1, :] / temperature\n",
        "            probs = torch.softmax(next_logits, dim=-1)\n",
        "\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            input_ids = torch.cat([input_ids, next_id], dim=1)\n",
        "            if next_id.item() == tokenizer.token2id.get(tokenizer.eos_token, -1):\n",
        "                break\n",
        "    output = tokenizer.decode(input_ids[0].tolist(), skip_special=True)\n",
        "    print(f\"You: {prompt}\\nBot: {output}\")\n"
      ],
      "metadata": {
        "id": "fd2AHJ9eROdJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate('Hello, how are you?', '/content/gpt_epoch2.pt', '/content/tokenizer.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--U-qHzJRcDd",
        "outputId": "572277e7-53b7-4741-a3cd-d48e82384fcf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Hello, how are you?\n",
            "Bot: H e ll o , h ow ar e you ?\n"
          ]
        }
      ]
    }
  ]
}